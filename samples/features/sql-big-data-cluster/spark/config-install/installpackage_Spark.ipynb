{
    "metadata": {
        "kernelspec": {
            "name": "sparkkernel",
            "display_name": "Spark | Scala"
        },
        "language_info": {
            "name": "scala",
            "mimetype": "text/x-scala",
            "codemirror_mode": "text/x-scala",
            "pygments_lexer": "scala"
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": "# Packaging in Spark\r\n",
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": "## Use Case 1: I can have key packages in boxed\r\n   - All pacakges that come with spark and hadoop distribution\r\n   - Python3.5 and Python 2.7\r\n   - Pandas, Sklearn and several other supporting ml packages\r\n   - R and supporting pacakges as part of MRO\r\n   - sparklyr\r\n\r\n   \r\n   ",
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": "## Use Case 2: I can install pacakges from maven repo to my spark cluster\r\nMaven central is a source of lot of packages. A lot of spark ecosystem pacakges are availble there. These pacakages can be installed to your spark cluster using notebook cell configuration at the start of your spark session.\r\n",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "%%configure -f\n{\"conf\": {\"spark.jars.packages\": \"com.microsoft.azure:azure-eventhubs-spark_2.11:2.3.1\"}}",
            "metadata": {
                "language": "scala"
            },
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "<IPython.core.display.HTML object>",
                        "text/html": "Current session configs: <tt>{'conf': {'spark.jars.packages': 'com.microsoft.azure:azure-eventhubs-spark_2.11:2.3.50'}, 'kind': 'spark'}</tt><br>"
                    },
                    "metadata": {}
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "<IPython.core.display.HTML object>",
                        "text/html": "No active sessions."
                    },
                    "metadata": {}
                }
            ],
            "execution_count": 3
        },
        {
            "cell_type": "code",
            "source": "import com.microsoft.azure.eventhubs._",
            "metadata": {},
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "import com.microsoft.azure.eventhubs._\n"
                }
            ],
            "execution_count": 5
        },
        {
            "cell_type": "markdown",
            "source": "## Use Case 3: I have a local jar that i want to run in the spark cluster\r\nAs a user you may build your own customer pacakges that want to run as part of your spark jobs. These pacakges can be uploaded as HDFS and using a notebook configuration spark can consume these pacakges in a jar.\r\n\r\n\r\n",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "%%configure -f\r\n    {\"conf\": {\"spark.jars\": \"/jar/mycodeJar.jar\"}}",
            "metadata": {},
            "outputs": [],
            "execution_count": 0
        },
        {
            "cell_type": "code",
            "source": "import com.my.mycodeJar._",
            "metadata": {},
            "outputs": [],
            "execution_count": 0
        }
    ]
}